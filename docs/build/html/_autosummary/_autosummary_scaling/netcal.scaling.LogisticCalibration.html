
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>netcal.scaling.LogisticCalibration &#8212; calibration-framework 1.2.0 documentation</title>
    <link rel="stylesheet" href="../../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="netcal.scaling.LogisticCalibrationDependent" href="netcal.scaling.LogisticCalibrationDependent.html" />
    <link rel="prev" title="netcal.scaling.AbstractLogisticRegression" href="netcal.scaling.AbstractLogisticRegression.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="netcal.scaling.LogisticCalibrationDependent.html" title="netcal.scaling.LogisticCalibrationDependent"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="netcal.scaling.AbstractLogisticRegression.html" title="netcal.scaling.AbstractLogisticRegression"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">calibration-framework 1.2.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../netcal.scaling.html" accesskey="U">netcal.scaling</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="netcal-scaling-logisticcalibration">
<h1>netcal.scaling.LogisticCalibration<a class="headerlink" href="#netcal-scaling-logisticcalibration" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="netcal.scaling.LogisticCalibration">
<em class="property">class </em><code class="sig-prename descclassname">netcal.scaling.</code><code class="sig-name descname">LogisticCalibration</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">temperature_only: bool = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">netcal.scaling.AbstractLogisticRegression.AbstractLogisticRegression</span></code></p>
<p>On classification, apply the logistic calibration method aka Platt scaling to obtain a
calibration mapping. This method is originally proposed by <a class="footnote-reference brackets" href="#id7" id="id1">1</a>.
For the multiclass case, we use the Vector scaling proposed in <a class="footnote-reference brackets" href="#id8" id="id2">2</a>.
On detection mode, this calibration method uses multiple independent normal distributions to obtain a
calibration mapping by means of the confidence as well as additional features <a class="footnote-reference brackets" href="#id10" id="id3">3</a>. This calibration scheme
assumes independence between all variables.</p>
<p>On detection, it is necessary to provide all data in input parameter <code class="docutils literal notranslate"><span class="pre">X</span></code> as an NumPy array
of shape <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></code>,
whereas the confidence must be the first feature given in the input array. The ground-truth samples <code class="docutils literal notranslate"><span class="pre">y</span></code>
must be an array of shape <code class="docutils literal notranslate"><span class="pre">(n_samples,)</span></code> consisting of binary labels <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>. Those
labels indicate if the according sample has matched a ground truth box <span class="math notranslate nohighlight">\(\text{m}=1\)</span> or is a false
prediction <span class="math notranslate nohighlight">\(\text{m}=0\)</span>.</p>
<p><strong>Mathematical background:</strong> For confidence calibration in classification tasks, a
confidence mapping <span class="math notranslate nohighlight">\(g\)</span> is applied on top of a miscalibrated scoring classifier <span class="math notranslate nohighlight">\(\hat{p} = h(x)\)</span> to
deliver a calibrated confidence score <span class="math notranslate nohighlight">\(\hat{q} = g(h(x))\)</span>.</p>
<p>For detection calibration, we can also use the additional box regression output which we denote as
<span class="math notranslate nohighlight">\(\hat{r} \in [0, 1]^J\)</span> with <span class="math notranslate nohighlight">\(J\)</span> as the number of dimensions used for the box encoding (e.g.
<span class="math notranslate nohighlight">\(J=4\)</span> for x position, y position, width and height).
Therefore, the calibration map is not only a function of the confidence score, but also of <span class="math notranslate nohighlight">\(\hat{r}\)</span>.
To define a general calibration map for binary problems, we use the logistic function and the combined
input <span class="math notranslate nohighlight">\(s = (\hat{p}, \hat{r})\)</span> of size K by</p>
<div class="math notranslate nohighlight">
\[g(s) = \frac{1}{1 + \exp(-z(s))} ,\]</div>
<p>According to <a class="footnote-reference brackets" href="#id7" id="id4">1</a>, we can interpret the logit <span class="math notranslate nohighlight">\(z\)</span> as the logarithm of the posterior odds</p>
<div class="math notranslate nohighlight">
\[z(s) = \log \frac{f(\text{m}=1 | s)}{f(\text{m}=0 | s)} \approx
\log \frac{f(s | \text{m}=1)}{f(s | \text{m}=1)} = \ell r(s)\]</div>
<p>If we assume independence of all variables given in <span class="math notranslate nohighlight">\(s\)</span>, we can use multiple univariate probability
density distributions with the same variance to obtain a calibration mapping. Using this formulation, we can
simply extend the scaling factor (from classification logistic calibration) to a scaling
vector <span class="math notranslate nohighlight">\(w \in \mathbb{R}^K\)</span>.
However, instead of using the uncalibrated confidence estimate <span class="math notranslate nohighlight">\(\hat{p}\)</span>, we use the logit of the
network as part of <span class="math notranslate nohighlight">\(s\)</span> to be conform with the original formulation in <a class="footnote-reference brackets" href="#id7" id="id5">1</a> and <a class="footnote-reference brackets" href="#id8" id="id6">2</a>. Thus,
the log-likelihood ratio can be expressed as</p>
<div class="math notranslate nohighlight">
\[\ell r(s) = s^T w + c,\]</div>
<p>with bias <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span>.
We utilize standard optimization methods to determine the calibration mapping <span class="math notranslate nohighlight">\(g(s)\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature_only</strong> (<em>bool</em><em>, </em><em>default: False</em>) – If True, use Temperature Scaling instead of Platt/Vector Scaling.</p></li>
<li><p><strong>method</strong> (<em>str</em><em>, </em><em>default: &quot;mle&quot;</em>) – Method that is used to obtain a calibration mapping:
- ‘mle’: Maximum likelihood estimate without uncertainty using a convex optimizer.
- ‘momentum’: MLE estimate using Momentum optimizer for non-convex optimization.
- ‘variational’: Variational Inference with uncertainty.
- ‘mcmc’: Markov-Chain Monte-Carlo sampling with uncertainty.</p></li>
<li><p><strong>momentum_epochs</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – Number of epochs used by momentum optimizer.</p></li>
<li><p><strong>mcmc_steps</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 20</em>) – Number of weight samples obtained by MCMC sampling.</p></li>
<li><p><strong>mcmc_chains</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1</em>) – Number of Markov-chains used in parallel for MCMC sampling (this will result
in mcmc_steps * mcmc_chains samples).</p></li>
<li><p><strong>mcmc_warmup_steps</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 100</em>) – Warmup steps used for MCMC sampling.</p></li>
<li><p><strong>vi_epochs</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – Number of epochs used for ELBO optimization.</p></li>
<li><p><strong>detection</strong> (<em>bool</em><em>, </em><em>default: False</em>) – If False, the input array ‘X’ is treated as multi-class confidence input (softmax)
with shape (n_samples, [n_classes]).
If True, the input array ‘X’ is treated as a box predictions with several box features (at least
box confidence must be present) with shape (n_samples, [n_box_features]).</p></li>
<li><p><strong>independent_probabilities</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Boolean for multi class probabilities.
If set to True, the probability estimates for each
class are treated as independent of each other (sigmoid).</p></li>
<li><p><strong>use_cuda</strong> (<em>str</em><em> or </em><em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Specify if CUDA should be used. If str, you can also specify the device
number like ‘cuda:0’, etc.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>,<a href="#id5">3</a>)</span></dt>
<dd><p>Platt, John:
“Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.”
Advances in large margin classifiers 10.3: 61-74, 1999
<a class="reference external" href="https://www.researchgate.net/profile/John_Platt/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods/links/004635154cff5262d6000000.pdf">Get source online</a></p>
</dd>
<dt class="label" id="id8"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Chuan Guo, Geoff Pleiss, Yu Sun and Kilian Q. Weinberger:
“On Calibration of Modern Neural Networks.”
Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.
<a class="reference external" href="https://arxiv.org/abs/1706.04599">Get source online</a></p>
</dd>
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Fabian Küppers, Jan Kronenberger, Amirhossein Shantia and Anselm Haselhoff:
“Multivariate Confidence Calibration for Object Detection.”
The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">4</span></dt>
<dd><p>Fabian Küppers, Jan Kronenberger, Jonas Schneider  and Anselm Haselhoff:
“Bayesian Confidence Calibration for Epistemic Uncertainty Modelling.”
2021 IEEE Intelligent Vehicles Symposium (IV), 2021</p>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code>(*args[, temperature_only])</p></td>
<td><p>Create an instance of <cite>LogisticCalibration</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">clear</span></code>()</p></td>
<td><p>Clear model parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">convex</span></code>(data, y, tensorboard, log_dir)</p></td>
<td><p>Convex optimization to find the global optimum of current parameter search.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code>(X, y[, random_state, tensorboard, log_dir])</p></td>
<td><p>Build logitic calibration model either conventional with single MLE estimate or with Variational Inference (VI) or Markov-Chain Monte-Carlo (MCMC) algorithm to also obtain uncertainty estimates.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code>(X[, y])</p></td>
<td><p>Fit to data, then transform it.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code>([deep])</p></td>
<td><p>Get parameters for this estimator.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">guide</span></code>([X, y])</p></td>
<td><p>Variational substitution definition for each parameter.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_model</span></code>(filename)</p></td>
<td><p>Load model from saved torch dump.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">mask</span></code>()</p></td>
<td><p>Seek for all relevant weights whose values are negative.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">mcmc</span></code>(data, y, tensorboard, log_dir)</p></td>
<td><p>Perform Markov-Chain Monte-Carlo sampling on the (unknown) posterior.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">model</span></code>([X, y])</p></td>
<td><p>Definition of the log regression model.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">momentum</span></code>(data, y, tensorboard, log_dir)</p></td>
<td><p>Momentum optimization to find the global optimum of current parameter search.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare</span></code>(X)</p></td>
<td><p>Preprocessing of input data before called at the beginning of the fit-function.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">prior</span></code>()</p></td>
<td><p>Prior definition of the weights used for log regression.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_model</span></code>(filename)</p></td>
<td><p>Save model instance as with torch’s save function as this is safer for torch tensors.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code>(**params)</p></td>
<td><p>Set the parameters of this estimator.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">squeeze_generic</span></code>(a, axes_to_keep)</p></td>
<td><p>Squeeze input array a but keep axes defined by parameter ‘axes_to_keep’ even if the dimension is of size 1.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code>(device)</p></td>
<td><p>Set distribution parameters to the desired device in order to compute either on CPU or GPU.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code>(X[, num_samples, random_state, …])</p></td>
<td><p>After model calibration, this function is used to get calibrated outputs of uncalibrated confidence estimates.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">variational</span></code>(data, y, tensorboard, log_dir)</p></td>
<td><p>Perform variational inference using the guide.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.clear">
<code class="sig-name descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear model parameters.</p>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.convex">
<code class="sig-name descname">convex</code><span class="sig-paren">(</span><em class="sig-param">data: torch.Tensor</em>, <em class="sig-param">y: torch.Tensor</em>, <em class="sig-param">tensorboard: bool</em>, <em class="sig-param">log_dir: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.convex" title="Permalink to this definition">¶</a></dt>
<dd><p>Convex optimization to find the global optimum of current parameter search.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_input</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – NumPy 2-D array with data input.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – NumPy array with ground truth labels as 1-D vector (binary).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X: numpy.ndarray</em>, <em class="sig-param">y: numpy.ndarray</em>, <em class="sig-param">random_state: int = None</em>, <em class="sig-param">tensorboard: bool = True</em>, <em class="sig-param">log_dir: str = None</em><span class="sig-paren">)</span> &#x2192; AbstractLogisticRegression<a class="headerlink" href="#netcal.scaling.LogisticCalibration.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Build logitic calibration model either conventional with single MLE estimate or with
Variational Inference (VI) or Markov-Chain Monte-Carlo (MCMC) algorithm to also obtain uncertainty estimates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_box_features</em><em>]</em><em>)</em>) – NumPy array with confidence values for each prediction on classification with shapes
1-D for binary classification, 2-D for multi class (softmax).
On detection, this array must have 2 dimensions with number of additional box features in last dim.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>)</em>) – NumPy array with ground truth labels.
Either as label vector (1-D) or as one-hot encoded ground truth array (2-D).</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – Fix the random seed for the random number</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Instance of class <a class="reference internal" href="netcal.scaling.AbstractLogisticRegression.html#netcal.scaling.AbstractLogisticRegression" title="netcal.scaling.AbstractLogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractLogisticRegression</span></code></a>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="netcal.scaling.AbstractLogisticRegression.html#netcal.scaling.AbstractLogisticRegression" title="netcal.scaling.AbstractLogisticRegression">AbstractLogisticRegression</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y=None</em>, <em class="sig-param">**fit_params</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Training set.</p></li>
<li><p><strong>y</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>]</em>) – Target values.</p></li>
<li><p><strong>**fit_params</strong> (<em>dict</em>) – Additional fit parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_new</strong> – Transformed array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape [n_samples, n_features_new]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>deep</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>params</strong> – Parameter names mapped to their values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>mapping of string to any</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.guide">
<code class="sig-name descname">guide</code><span class="sig-paren">(</span><em class="sig-param">X: torch.Tensor = None</em>, <em class="sig-param">y: torch.Tensor = None</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.guide" title="Permalink to this definition">¶</a></dt>
<dd><p>Variational substitution definition for each parameter. The signature is the same as for the
“self.model” function but the variables are not used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>torch.Tensor</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_log_regression_features</em><em>)</em>) – Input data that has been prepared by “self.prepare” function call.</p></li>
<li><p><strong>y</strong> (<em>torch.Tensor</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>)</em>) – Torch tensor with ground truth labels.
Either as label vector (1-D) or as one-hot encoded ground truth array (2-D) (for multiclass MLE only).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.intercept">
<em class="property">property </em><code class="sig-name descname">intercept</code><a class="headerlink" href="#netcal.scaling.LogisticCalibration.intercept" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for intercept of logistic calibration.</p>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">filename</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load model from saved torch dump.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filename</strong> (<em>str</em>) – String with filename.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Instance of a child class of <cite>AbstractCalibration</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../../_autosummary_abstract_calibration/netcal.AbstractCalibration.html#netcal.AbstractCalibration" title="netcal.AbstractCalibration">AbstractCalibration</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.mask">
<code class="sig-name descname">mask</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tuple[numpy.ndarray, List]<a class="headerlink" href="#netcal.scaling.LogisticCalibration.mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Seek for all relevant weights whose values are negative. Mask those values with optimization constraints
in the interval [0, 0].
Constraints on the intercepts might also be set.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Indices of masked values and list of boundary constraints for optimization.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tuple of (np.ndarray, list)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.mcmc">
<code class="sig-name descname">mcmc</code><span class="sig-paren">(</span><em class="sig-param">data: torch.Tensor</em>, <em class="sig-param">y: torch.Tensor</em>, <em class="sig-param">tensorboard: bool</em>, <em class="sig-param">log_dir: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.mcmc" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Markov-Chain Monte-Carlo sampling on the (unknown) posterior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_input</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – NumPy 2-D array with data input.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – NumPy array with ground truth labels as 1-D vector (binary).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.model">
<code class="sig-name descname">model</code><span class="sig-paren">(</span><em class="sig-param">X: torch.Tensor = None</em>, <em class="sig-param">y: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#netcal.scaling.LogisticCalibration.model" title="Permalink to this definition">¶</a></dt>
<dd><p>Definition of the log regression model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>torch.Tensor</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_log_regression_features</em><em>)</em>) – Input data that has been prepared by “self.prepare” function call.</p></li>
<li><p><strong>y</strong> (<em>torch.Tensor</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>)</em>) – Torch tensor with ground truth labels.
Either as label vector (1-D) or as one-hot encoded ground truth array (2-D) (for multiclass MLE only).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Logit of the log regression model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor, shape=(n_samples, [n_classes])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.momentum">
<code class="sig-name descname">momentum</code><span class="sig-paren">(</span><em class="sig-param">data: torch.Tensor</em>, <em class="sig-param">y: torch.Tensor</em>, <em class="sig-param">tensorboard: bool</em>, <em class="sig-param">log_dir: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.momentum" title="Permalink to this definition">¶</a></dt>
<dd><p>Momentum optimization to find the global optimum of current parameter search.
This method is slow but tends to find the global optimum for non-convex optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_input</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – NumPy 2-D array with data input.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – NumPy array with ground truth labels as 1-D vector (binary).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.prepare">
<code class="sig-name descname">prepare</code><span class="sig-paren">(</span><em class="sig-param">X: numpy.ndarray</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#netcal.scaling.LogisticCalibration.prepare" title="Permalink to this definition">¶</a></dt>
<dd><p>Preprocessing of input data before called at the beginning of the fit-function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_box_features</em><em>]</em><em>)</em>) – NumPy array with confidence values for each prediction on classification with shapes
1-D for binary classification, 2-D for multi class (softmax).
On detection, this array must have 2 dimensions with number of additional box features in last dim.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Prepared data vector X as torch tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.prior">
<code class="sig-name descname">prior</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.prior" title="Permalink to this definition">¶</a></dt>
<dd><p>Prior definition of the weights used for log regression. This function has to set the
variables ‘self.weight_prior_dist’, ‘self.weight_mean_init’ and ‘self.weight_stddev_init’.</p>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">filename: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save model instance as with torch’s save function as this is safer for torch tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filename</strong> (<em>str</em>) – String with filename.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">**params</em><span class="sig-paren">)</span> &#x2192; netcal.AbstractCalibration.AbstractCalibration<a class="headerlink" href="#netcal.scaling.LogisticCalibration.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.squeeze_generic">
<em class="property">classmethod </em><code class="sig-name descname">squeeze_generic</code><span class="sig-paren">(</span><em class="sig-param">a: numpy.ndarray, axes_to_keep: Union[int, Iterable[int]]</em><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="headerlink" href="#netcal.scaling.LogisticCalibration.squeeze_generic" title="Permalink to this definition">¶</a></dt>
<dd><p>Squeeze input array a but keep axes defined by parameter ‘axes_to_keep’ even if the dimension is
of size 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<em>np.ndarray</em>) – NumPy array that should be squeezed.</p></li>
<li><p><strong>axes_to_keep</strong> (<em>int</em><em> or </em><em>iterable</em>) – Axes that should be kept even if they have a size of 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Squeezed array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">device: torch.device</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Set distribution parameters to the desired device in order to compute either on CPU or GPU.</p>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">X: numpy.ndarray</em>, <em class="sig-param">num_samples: int = 1000</em>, <em class="sig-param">random_state: int = None</em>, <em class="sig-param">mean_estimate: bool = False</em><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="headerlink" href="#netcal.scaling.LogisticCalibration.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>After model calibration, this function is used to get calibrated outputs of uncalibrated
confidence estimates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_box_features</em><em>]</em><em>)</em>) – NumPy array with confidence values for each prediction on classification with shapes
1-D for binary classification, 2-D for multi class (softmax).
On detection, this array must have 2 dimensions with number of additional box features in last dim.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – Number of samples generated on MCMC sampling or Variational Inference.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – Fix the random seed for the random number</p></li>
<li><p><strong>mean_estimate</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – If True, directly return the mean on probabilistic methods like MCMC or VI instead of the full
distribution. This parameter has no effect on MLE.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>np.ndarray, shape=(n_samples, [n_classes]) on MLE or on MCMC/VI if ‘mean_estimate’ is True</em></p></li>
<li><p><em>or shape=(n_parameters, n_samples, [n_classes]) on VI, MCMC if ‘mean_estimate’ is False</em> – On MLE without uncertainty, return NumPy array with calibrated confidence estimates.
1-D for binary classification, 2-D for multi class (softmax).
On VI or MCMC, return NumPy array with leading dimension as the number of sampled parameters from the
log regression parameter distribution obtained by VI or MCMC.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.variational">
<code class="sig-name descname">variational</code><span class="sig-paren">(</span><em class="sig-param">data: torch.Tensor</em>, <em class="sig-param">y: torch.Tensor</em>, <em class="sig-param">tensorboard: bool</em>, <em class="sig-param">log_dir: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.LogisticCalibration.variational" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform variational inference using the guide.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_input</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – NumPy 2-D array with data input.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – NumPy array with ground truth labels as 1-D vector (binary).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.LogisticCalibration.weights">
<em class="property">property </em><code class="sig-name descname">weights</code><a class="headerlink" href="#netcal.scaling.LogisticCalibration.weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter for weights of logistic calibration.</p>
</dd></dl>

</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="netcal.scaling.AbstractLogisticRegression.html"
                        title="previous chapter">netcal.scaling.AbstractLogisticRegression</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="netcal.scaling.LogisticCalibrationDependent.html"
                        title="next chapter">netcal.scaling.LogisticCalibrationDependent</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/_autosummary/_autosummary_scaling/netcal.scaling.LogisticCalibration.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="netcal.scaling.LogisticCalibrationDependent.html" title="netcal.scaling.LogisticCalibrationDependent"
             >next</a> |</li>
        <li class="right" >
          <a href="netcal.scaling.AbstractLogisticRegression.html" title="netcal.scaling.AbstractLogisticRegression"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">calibration-framework 1.2.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../netcal.scaling.html" >netcal.scaling</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019-2021, Ruhr West University of Applied Sciences, Bottrop, Germany AND Elektronische Fahrwerksysteme GmbH, Gaimersheim Germany.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.4.
    </div>
  </body>
</html>