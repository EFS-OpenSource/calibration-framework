
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>netcal.scaling.AbstractLogisticRegression &#8212; calibration-framework 1.2.1 documentation</title>
    <link rel="stylesheet" href="../../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="netcal.scaling.LogisticCalibration" href="netcal.scaling.LogisticCalibration.html" />
    <link rel="prev" title="netcal.scaling" href="../netcal.scaling.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="netcal.scaling.LogisticCalibration.html" title="netcal.scaling.LogisticCalibration"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../netcal.scaling.html" title="netcal.scaling"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">calibration-framework 1.2.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../netcal.scaling.html" accesskey="U">netcal.scaling</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="netcal-scaling-abstractlogisticregression">
<h1>netcal.scaling.AbstractLogisticRegression<a class="headerlink" href="#netcal-scaling-abstractlogisticregression" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="netcal.scaling.AbstractLogisticRegression">
<em class="property">class </em><code class="sig-prename descclassname">netcal.scaling.</code><code class="sig-name descname">AbstractLogisticRegression</code><span class="sig-paren">(</span><em class="sig-param">method: str = 'mle'</em>, <em class="sig-param">momentum_epochs: int = 1000</em>, <em class="sig-param">mcmc_steps: int = 250</em>, <em class="sig-param">mcmc_chains: int = 1</em>, <em class="sig-param">mcmc_warmup_steps: int = 100</em>, <em class="sig-param">vi_epochs: int = 1000</em>, <em class="sig-param">detection: bool = False</em>, <em class="sig-param">independent_probabilities: bool = False</em>, <em class="sig-param">use_cuda: Union[str</em>, <em class="sig-param">bool] = False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">netcal.AbstractCalibration.AbstractCalibration</span></code></p>
<p>Abstract class for all calibration methods that base on logistic regression. We extended common
scaling calibration methods by Bayesian epistemic uncertainty modelling <a class="footnote-reference brackets" href="#id3" id="id1">1</a>.
On the one hand, this class supports Maximum Likelihood (MLE) estimates without uncertainty.
This method is commonly solved by negative log likelihood optimization given by</p>
<div class="math notranslate nohighlight">
\[\theta_\text{MLE} = \underset{\theta}{\text{min}} \, -\sum_{i=1}^N \log p(y | x_i, \theta)\]</div>
<p>with samples <span class="math notranslate nohighlight">\(X\)</span>, label <span class="math notranslate nohighlight">\(y\)</span>, weights <span class="math notranslate nohighlight">\(\theta\)</span> and likelihood <span class="math notranslate nohighlight">\(p(y|X, \theta)\)</span>.
See the implementations of the methods for more details.</p>
<p>On the other hand, methods to obtain uncertainty in calibration are currently Variational Inference (VI) and
Markov-Chain Monte-Carlo (MCMC) sampling. Instead of estimating the weights <span class="math notranslate nohighlight">\(\theta\)</span> of the logistic
regression directly, we place a probability distribution over the weights by</p>
<div class="math notranslate nohighlight">
\[p(\theta | X, y) = \frac{p(y | X, \theta) p(\theta)}{\int p(y | X, \theta) p(\theta) d\theta}\]</div>
<p>Since the marginal likelihood cannot be evaluated analytically for logistic regression, we need to approximate the
posterior by either MCMC sampling or Variational Inference. Using several techniques, we sample multiple times from
the posterior in order to get multiple related calibration results with a mean and a deviation for each sample.</p>
<p>MCMC sampling allows the sampling of a posterior without knowing the marginal likelihood. This method is unbiased
but computational expensive. In contrast, Variational Inference defines an easy variational
distribution <span class="math notranslate nohighlight">\(q_\Phi(\theta)\)</span> (e.g. a normal distribution) for each weight parametrized by <span class="math notranslate nohighlight">\(\Phi\)</span>.
The optimization objective is then the minimization of the Kullback-Leibler divergence between the
variational distribution <span class="math notranslate nohighlight">\(q_\Phi(\theta))\)</span> and the true posterior <span class="math notranslate nohighlight">\(p(\theta | X, y)\)</span>.
This can be solved using the ELBO method <a class="footnote-reference brackets" href="#id4" id="id2">2</a>. Variational Inference is faster than MCMC but also biased.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>method</strong> (<em>str</em><em>, </em><em>default: &quot;mle&quot;</em>) – Method that is used to obtain a calibration mapping:
- ‘mle’: Maximum likelihood estimate without uncertainty using a convex optimizer.
- ‘momentum’: MLE estimate using Momentum optimizer for non-convex optimization.
- ‘variational’: Variational Inference with uncertainty.
- ‘mcmc’: Markov-Chain Monte-Carlo sampling with uncertainty.</p></li>
<li><p><strong>momentum_epochs</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – Number of epochs used by momentum optimizer.</p></li>
<li><p><strong>mcmc_steps</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 20</em>) – Number of weight samples obtained by MCMC sampling.</p></li>
<li><p><strong>mcmc_chains</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1</em>) – Number of Markov-chains used in parallel for MCMC sampling (this will result
in mcmc_steps * mcmc_chains samples).</p></li>
<li><p><strong>mcmc_warmup_steps</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 100</em>) – Warmup steps used for MCMC sampling.</p></li>
<li><p><strong>vi_epochs</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – Number of epochs used for ELBO optimization.</p></li>
<li><p><strong>detection</strong> (<em>bool</em><em>, </em><em>default: False</em>) – If False, the input array ‘X’ is treated as multi-class confidence input (softmax)
with shape (n_samples, [n_classes]).
If True, the input array ‘X’ is treated as a box predictions with several box features (at least
box confidence must be present) with shape (n_samples, [n_box_features]).</p></li>
<li><p><strong>independent_probabilities</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Boolean for multi class probabilities.
If set to True, the probability estimates for each
class are treated as independent of each other (sigmoid).</p></li>
<li><p><strong>use_cuda</strong> (<em>str</em><em> or </em><em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – Specify if CUDA should be used. If str, you can also specify the device
number like ‘cuda:0’, etc.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Fabian Küppers, Jan Kronenberger, Jonas Schneider  and Anselm Haselhoff:
“Bayesian Confidence Calibration for Epistemic Uncertainty Modelling.”
2021 IEEE Intelligent Vehicles Symposium (IV), 2021</p>
</dd>
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul:
“An introduction to variational methods for graphical models.” Machine learning, 37(2): 183–233, 1999.</p>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code>([method, momentum_epochs, …])</p></td>
<td><p>Create an instance of <cite>AbstractLogisticRegression</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">clear</span></code>()</p></td>
<td><p>Clear model parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">convex</span></code>(data, y, tensorboard, log_dir)</p></td>
<td><p>Convex optimization to find the global optimum of current parameter search.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code>(X, y[, random_state, tensorboard, log_dir])</p></td>
<td><p>Build logitic calibration model either conventional with single MLE estimate or with Variational Inference (VI) or Markov-Chain Monte-Carlo (MCMC) algorithm to also obtain uncertainty estimates.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit_transform</span></code>(X[, y])</p></td>
<td><p>Fit to data, then transform it.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code>([deep])</p></td>
<td><p>Get parameters for this estimator.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">guide</span></code>([X, y])</p></td>
<td><p>Variational substitution definition for each parameter.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_model</span></code>(filename)</p></td>
<td><p>Load model from saved torch dump.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">mask</span></code>()</p></td>
<td><p>Seek for all relevant weights whose values are negative.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">mcmc</span></code>(data, y, tensorboard, log_dir)</p></td>
<td><p>Perform Markov-Chain Monte-Carlo sampling on the (unknown) posterior.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">model</span></code>([X, y])</p></td>
<td><p>Definition of the log regression model.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">momentum</span></code>(data, y, tensorboard, log_dir)</p></td>
<td><p>Momentum optimization to find the global optimum of current parameter search.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare</span></code>(X)</p></td>
<td><p>Preprocessing of input data before called at the beginning of the fit-function.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">prior</span></code>()</p></td>
<td><p>Prior definition of the weights and intercept used for log regression.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_model</span></code>(filename)</p></td>
<td><p>Save model instance as with torch’s save function as this is safer for torch tensors.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code>(**params)</p></td>
<td><p>Set the parameters of this estimator.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">squeeze_generic</span></code>(a, axes_to_keep)</p></td>
<td><p>Squeeze input array a but keep axes defined by parameter ‘axes_to_keep’ even if the dimension is of size 1.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code>(device)</p></td>
<td><p>Set distribution parameters to the desired device in order to compute either on CPU or GPU.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code>(X[, num_samples, random_state, …])</p></td>
<td><p>After model calibration, this function is used to get calibrated outputs of uncalibrated confidence estimates.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">variational</span></code>(data, y, tensorboard, log_dir)</p></td>
<td><p>Perform variational inference using the guide.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.clear">
<code class="sig-name descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear model parameters.</p>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.convex">
<code class="sig-name descname">convex</code><span class="sig-paren">(</span><em class="sig-param">data: torch.Tensor</em>, <em class="sig-param">y: torch.Tensor</em>, <em class="sig-param">tensorboard: bool</em>, <em class="sig-param">log_dir: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.convex" title="Permalink to this definition">¶</a></dt>
<dd><p>Convex optimization to find the global optimum of current parameter search.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_input</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – NumPy 2-D array with data input.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – NumPy array with ground truth labels as 1-D vector (binary).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X: numpy.ndarray</em>, <em class="sig-param">y: numpy.ndarray</em>, <em class="sig-param">random_state: int = None</em>, <em class="sig-param">tensorboard: bool = True</em>, <em class="sig-param">log_dir: str = None</em><span class="sig-paren">)</span> &#x2192; AbstractLogisticRegression<a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Build logitic calibration model either conventional with single MLE estimate or with
Variational Inference (VI) or Markov-Chain Monte-Carlo (MCMC) algorithm to also obtain uncertainty estimates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_box_features</em><em>]</em><em>)</em>) – NumPy array with confidence values for each prediction on classification with shapes
1-D for binary classification, 2-D for multi class (softmax).
On detection, this array must have 2 dimensions with number of additional box features in last dim.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>)</em>) – NumPy array with ground truth labels.
Either as label vector (1-D) or as one-hot encoded ground truth array (2-D).</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – Fix the random seed for the random number</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Instance of class <a class="reference internal" href="#netcal.scaling.AbstractLogisticRegression" title="netcal.scaling.AbstractLogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractLogisticRegression</span></code></a>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#netcal.scaling.AbstractLogisticRegression" title="netcal.scaling.AbstractLogisticRegression">AbstractLogisticRegression</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y=None</em>, <em class="sig-param">**fit_params</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Training set.</p></li>
<li><p><strong>y</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>]</em>) – Target values.</p></li>
<li><p><strong>**fit_params</strong> (<em>dict</em>) – Additional fit parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_new</strong> – Transformed array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array of shape [n_samples, n_features_new]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>deep</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>params</strong> – Parameter names mapped to their values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>mapping of string to any</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.guide">
<code class="sig-name descname">guide</code><span class="sig-paren">(</span><em class="sig-param">X: torch.Tensor = None</em>, <em class="sig-param">y: torch.Tensor = None</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.guide" title="Permalink to this definition">¶</a></dt>
<dd><p>Variational substitution definition for each parameter. The signature is the same as for the
“self.model” function but the variables are not used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>torch.Tensor</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_log_regression_features</em><em>)</em>) – Input data that has been prepared by “self.prepare” function call.</p></li>
<li><p><strong>y</strong> (<em>torch.Tensor</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>)</em>) – Torch tensor with ground truth labels.
Either as label vector (1-D) or as one-hot encoded ground truth array (2-D) (for multiclass MLE only).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.load_model">
<code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param">filename</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load model from saved torch dump.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filename</strong> (<em>str</em>) – String with filename.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Instance of a child class of <cite>AbstractCalibration</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../../_autosummary_abstract_calibration/netcal.AbstractCalibration.html#netcal.AbstractCalibration" title="netcal.AbstractCalibration">AbstractCalibration</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.mask">
<code class="sig-name descname">mask</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tuple[numpy.ndarray, List]<a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Seek for all relevant weights whose values are negative. Mask those values with optimization constraints
in the interval [0, 0].
Constraints on the intercepts might also be set.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Indices of masked values and list of boundary constraints for optimization.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tuple of (np.ndarray, list)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.mcmc">
<code class="sig-name descname">mcmc</code><span class="sig-paren">(</span><em class="sig-param">data: torch.Tensor</em>, <em class="sig-param">y: torch.Tensor</em>, <em class="sig-param">tensorboard: bool</em>, <em class="sig-param">log_dir: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.mcmc" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Markov-Chain Monte-Carlo sampling on the (unknown) posterior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_input</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – NumPy 2-D array with data input.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – NumPy array with ground truth labels as 1-D vector (binary).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.model">
<em class="property">abstract </em><code class="sig-name descname">model</code><span class="sig-paren">(</span><em class="sig-param">X: torch.Tensor = None</em>, <em class="sig-param">y: torch.Tensor = None</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.model" title="Permalink to this definition">¶</a></dt>
<dd><p>Definition of the log regression model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>torch.Tensor</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_log_regression_features</em><em>)</em>) – Input data that has been prepared by “self.prepare” function call.</p></li>
<li><p><strong>y</strong> (<em>torch.Tensor</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>)</em>) – Torch tensor with ground truth labels.
Either as label vector (1-D) or as one-hot encoded ground truth array (2-D) (for multiclass MLE only).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Logit of the log regression model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor, shape=(n_samples, [n_classes])</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.momentum">
<code class="sig-name descname">momentum</code><span class="sig-paren">(</span><em class="sig-param">data: torch.Tensor</em>, <em class="sig-param">y: torch.Tensor</em>, <em class="sig-param">tensorboard: bool</em>, <em class="sig-param">log_dir: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.momentum" title="Permalink to this definition">¶</a></dt>
<dd><p>Momentum optimization to find the global optimum of current parameter search.
This method is slow but tends to find the global optimum for non-convex optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_input</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – NumPy 2-D array with data input.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – NumPy array with ground truth labels as 1-D vector (binary).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.prepare">
<em class="property">abstract </em><code class="sig-name descname">prepare</code><span class="sig-paren">(</span><em class="sig-param">X: numpy.ndarray</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.prepare" title="Permalink to this definition">¶</a></dt>
<dd><p>Preprocessing of input data before called at the beginning of the fit-function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_box_features</em><em>]</em><em>)</em>) – NumPy array with confidence values for each prediction on classification with shapes
1-D for binary classification, 2-D for multi class (softmax).
On detection, this array must have 2 dimensions with number of additional box features in last dim.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Prepared data vector X as torch tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.prior">
<em class="property">abstract </em><code class="sig-name descname">prior</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.prior" title="Permalink to this definition">¶</a></dt>
<dd><p>Prior definition of the weights and intercept used for log regression. This function has to set the
sites at least for “weights” and “bias”.</p>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.save_model">
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">filename: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save model instance as with torch’s save function as this is safer for torch tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filename</strong> (<em>str</em>) – String with filename.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">**params</em><span class="sig-paren">)</span> &#x2192; netcal.AbstractCalibration.AbstractCalibration<a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.squeeze_generic">
<em class="property">classmethod </em><code class="sig-name descname">squeeze_generic</code><span class="sig-paren">(</span><em class="sig-param">a: numpy.ndarray, axes_to_keep: Union[int, Iterable[int]]</em><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.squeeze_generic" title="Permalink to this definition">¶</a></dt>
<dd><p>Squeeze input array a but keep axes defined by parameter ‘axes_to_keep’ even if the dimension is
of size 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<em>np.ndarray</em>) – NumPy array that should be squeezed.</p></li>
<li><p><strong>axes_to_keep</strong> (<em>int</em><em> or </em><em>iterable</em>) – Axes that should be kept even if they have a size of 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Squeezed array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">device: torch.device</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Set distribution parameters to the desired device in order to compute either on CPU or GPU.</p>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">X: numpy.ndarray</em>, <em class="sig-param">num_samples: int = 1000</em>, <em class="sig-param">random_state: int = None</em>, <em class="sig-param">mean_estimate: bool = False</em><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>After model calibration, this function is used to get calibrated outputs of uncalibrated
confidence estimates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_classes</em><em>]</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>[</em><em>n_box_features</em><em>]</em><em>)</em>) – NumPy array with confidence values for each prediction on classification with shapes
1-D for binary classification, 2-D for multi class (softmax).
On detection, this array must have 2 dimensions with number of additional box features in last dim.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: 1000</em>) – Number of samples generated on MCMC sampling or Variational Inference.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default: None</em>) – Fix the random seed for the random number</p></li>
<li><p><strong>mean_estimate</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default: False</em>) – If True, directly return the mean on probabilistic methods like MCMC or VI instead of the full
distribution. This parameter has no effect on MLE.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>np.ndarray, shape=(n_samples, [n_classes]) on MLE or on MCMC/VI if ‘mean_estimate’ is True</em></p></li>
<li><p><em>or shape=(n_parameters, n_samples, [n_classes]) on VI, MCMC if ‘mean_estimate’ is False</em> – On MLE without uncertainty, return NumPy array with calibrated confidence estimates.
1-D for binary classification, 2-D for multi class (softmax).
On VI or MCMC, return NumPy array with leading dimension as the number of sampled parameters from the
log regression parameter distribution obtained by VI or MCMC.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="netcal.scaling.AbstractLogisticRegression.variational">
<code class="sig-name descname">variational</code><span class="sig-paren">(</span><em class="sig-param">data: torch.Tensor</em>, <em class="sig-param">y: torch.Tensor</em>, <em class="sig-param">tensorboard: bool</em>, <em class="sig-param">log_dir: str</em><span class="sig-paren">)</span><a class="headerlink" href="#netcal.scaling.AbstractLogisticRegression.variational" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform variational inference using the guide.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_input</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – NumPy 2-D array with data input.</p></li>
<li><p><strong>y</strong> (<em>np.ndarray</em><em>, </em><em>shape=</em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – NumPy array with ground truth labels as 1-D vector (binary).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="../netcal.scaling.html"
                        title="previous chapter">netcal.scaling</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="netcal.scaling.LogisticCalibration.html"
                        title="next chapter">netcal.scaling.LogisticCalibration</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/_autosummary/_autosummary_scaling/netcal.scaling.AbstractLogisticRegression.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="netcal.scaling.LogisticCalibration.html" title="netcal.scaling.LogisticCalibration"
             >next</a> |</li>
        <li class="right" >
          <a href="../netcal.scaling.html" title="netcal.scaling"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">calibration-framework 1.2.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../netcal.scaling.html" >netcal.scaling</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019-2021, Ruhr West University of Applied Sciences, Bottrop, Germany AND Elektronische Fahrwerksysteme GmbH, Gaimersheim Germany.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.4.
    </div>
  </body>
</html>