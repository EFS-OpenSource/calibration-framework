
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>API Reference &#8212; calibration-framework 1.1.1 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="netcal.binning" href="_autosummary/netcal.binning.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="_autosummary/netcal.binning.html" title="netcal.binning"
             accesskey="N">next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">calibration-framework 1.1.1 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <span class="target" id="module-netcal"></span><div class="section" id="api-reference">
<h1><a class="toc-backref" href="#id47">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this headline">¶</a></h1>
<p>This is the detailled API reference for the confidence calibration framework. This framework can be used to
obtain well-calibrated confidence estimates from biased estimators like Neural Networks.
The API reference contains a detailled description of all available methods and their parameters. For
miscellaneous examples on how to use these methods, see readme below.</p>
<p>Available packages</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="_autosummary/netcal.binning.html#module-netcal.binning" title="netcal.binning"><code class="xref py py-obj docutils literal notranslate"><span class="pre">binning</span></code></a></p></td>
<td><p>Binning methods for confidence calibration.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="_autosummary/netcal.scaling.html#module-netcal.scaling" title="netcal.scaling"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scaling</span></code></a></p></td>
<td><p>Scaling methods for confidence calibration.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="_autosummary/netcal.regularization.html#module-netcal.regularization" title="netcal.regularization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">regularization</span></code></a></p></td>
<td><p>Regularization methods which are applied during model training.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="_autosummary/netcal.metrics.html#module-netcal.metrics" title="netcal.metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">metrics</span></code></a></p></td>
<td><p>Methods for measuring miscalibration.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="_autosummary/netcal.presentation.html#module-netcal.presentation" title="netcal.presentation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">presentation</span></code></a></p></td>
<td><p>Methods for the visualization of miscalibration.</p></td>
</tr>
</tbody>
</table>
<p>Each calibration method must inherit the base class <a class="reference internal" href="_autosummary_abstract_calibration/netcal.AbstractCalibration.html#netcal.AbstractCalibration" title="netcal.AbstractCalibration"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractCalibration</span></code></a>. If you want to write your own method and
include into the framework, include this class as the base class.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="_autosummary_abstract_calibration/netcal.AbstractCalibration.html#netcal.AbstractCalibration" title="netcal.AbstractCalibration"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AbstractCalibration</span></code></a>([detection, …])</p></td>
<td><p>Abstract base class for all calibration methods.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="calibration-framework">
<h1><a class="toc-backref" href="#id48">Calibration Framework</a><a class="headerlink" href="#calibration-framework" title="Permalink to this headline">¶</a></h1>
<p>Calibration framework in Python 3 for Neural Networks.
For full API reference documentation, visit <a class="reference external" href="https://fabiankueppers.github.io/calibration-framework">https://fabiankueppers.github.io/calibration-framework</a>.</p>
<p>Copyright (C) 2019-2020 Ruhr West University of Applied Sciences, Bottrop, Germany
AND Visteon Electronics Germany GmbH, Kerpen, Germany</p>
<p>This Source Code Form is subject to the terms of the Mozilla Public
License, v. 2.0. If a copy of the MPL was not distributed with this
file, You can obtain one at <a class="reference external" href="http://mozilla.org/MPL/2.0/">http://mozilla.org/MPL/2.0/</a>.</p>
<p>If you use this framework or parts of it for your research, please cite it by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@InProceedings</span><span class="p">{</span><span class="n">Kueppers_2020_CVPR_Workshops</span><span class="p">,</span>
   <span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">Küppers</span><span class="p">,</span> <span class="n">Fabian</span> <span class="ow">and</span> <span class="n">Kronenberger</span><span class="p">,</span> <span class="n">Jan</span> <span class="ow">and</span> <span class="n">Shantia</span><span class="p">,</span> <span class="n">Amirhossein</span> <span class="ow">and</span> <span class="n">Haselhoff</span><span class="p">,</span> <span class="n">Anselm</span><span class="p">},</span>
   <span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">Multivariate</span> <span class="n">Confidence</span> <span class="n">Calibration</span> <span class="k">for</span> <span class="n">Object</span> <span class="n">Detection</span><span class="p">},</span>
   <span class="n">booktitle</span> <span class="o">=</span> <span class="p">{</span><span class="n">The</span> <span class="n">IEEE</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Computer</span> <span class="n">Vision</span> <span class="ow">and</span> <span class="n">Pattern</span> <span class="n">Recognition</span> <span class="p">(</span><span class="n">CVPR</span><span class="p">)</span> <span class="n">Workshops</span><span class="p">,</span> <span class="ow">in</span> <span class="n">press</span><span class="p">},</span>
   <span class="n">month</span> <span class="o">=</span> <span class="p">{</span><span class="n">June</span><span class="p">},</span>
   <span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2020</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="contents topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#api-reference" id="id47">API Reference</a></p></li>
<li><p><a class="reference internal" href="#calibration-framework" id="id48">Calibration Framework</a></p></li>
<li><p><a class="reference internal" href="#overview" id="id49">Overview</a></p></li>
<li><p><a class="reference internal" href="#installation" id="id50">Installation</a></p>
<ul>
<li><p><a class="reference internal" href="#requirements" id="id51">Requirements</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#calibration-metrics" id="id52">Calibration Metrics</a></p></li>
<li><p><a class="reference internal" href="#methods" id="id53">Methods</a></p>
<ul>
<li><p><a class="reference internal" href="#binning" id="id54">Binning</a></p></li>
<li><p><a class="reference internal" href="#scaling" id="id55">Scaling</a></p></li>
<li><p><a class="reference internal" href="#regularization" id="id56">Regularization</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#visualization" id="id57">Visualization</a></p></li>
<li><p><a class="reference internal" href="#examples" id="id58">Examples</a></p>
<ul>
<li><p><a class="reference internal" href="#classification" id="id59">Classification</a></p></li>
<li><p><a class="reference internal" href="#detection" id="id60">Detection</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references" id="id61">References</a></p></li>
</ul>
</div>
</div>
<div class="section" id="overview">
<h1><a class="toc-backref" href="#id49">Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h1>
<p>This framework is designed to calibrate the confidence estimates of classifiers like neural networks. Modern neural networks are likely to be overconfident with their predictions. However, reliable confidence estimates of such classifiers are crucial especially in safety-critical applications.</p>
<p>For example: given 100 predictions with a confidence of 80% of each prediction, the observed accuracy should also match 80% (neither more nor less). This behaviour is achievable with several calibration methods.</p>
<p>This framework can also be used to calibrate object detection models. It has recently been shown that calibration on object detection also depends on the position and/or scale of a predicted object <a class="footnote-reference brackets" href="#id46" id="id1">12</a>. We provide calibration methods to perform confidence calibration w.r.t. the additional box regression branch.
For this purpose, we extended the commonly used Histogram Binning <a class="footnote-reference brackets" href="#id37" id="id2">3</a>, Logistic Calibration alias Platt scaling <a class="footnote-reference brackets" href="#id44" id="id3">10</a> and the Beta Calibration method <a class="footnote-reference brackets" href="#id36" id="id4">2</a> to also include the bounding box information into a calibration mapping.
Furthermore, we provide two new methods called the <em>Dependent Logistic Calibration</em> and the <em>Dependent Beta Calibration</em> that are not only able to perform a calibration mapping
w.r.t. additional bounding box information but also to model correlations and dependencies between all given quantities <a class="footnote-reference brackets" href="#id46" id="id5">12</a>. Those methods should be preffered over their counterparts in object detection mode.</p>
<p>The framework is structured as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">netcal</span>
  <span class="o">.</span><span class="n">binning</span>         <span class="c1"># binning methods</span>
  <span class="o">.</span><span class="n">scaling</span>         <span class="c1"># scaling methods</span>
  <span class="o">.</span><span class="n">regularization</span>  <span class="c1"># regularization methods</span>
  <span class="o">.</span><span class="n">presentation</span>    <span class="c1"># presentation methods</span>
  <span class="o">.</span><span class="n">metrics</span>         <span class="c1"># metrics for measuring miscalibration</span>

<span class="n">examples</span>           <span class="c1"># example code snippets</span>
</pre></div>
</div>
</div>
<div class="section" id="installation">
<h1><a class="toc-backref" href="#id50">Installation</a><a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h1>
<p>The installation of the calibration suite is quite easy with setuptools. You can either install this framework using PIP:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip3</span> <span class="n">install</span> <span class="n">netcal</span>
</pre></div>
</div>
<p>Or simply invoke the following command to install the calibration suite:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">install</span>
</pre></div>
</div>
<div class="section" id="requirements">
<h2><a class="toc-backref" href="#id51">Requirements</a><a class="headerlink" href="#requirements" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>numpy&gt;=1.15</p></li>
<li><p>scipy&gt;=1.3</p></li>
<li><p>matplotlib&gt;=3.1</p></li>
<li><p>scikit-learn&gt;=0.20.0</p></li>
<li><p>torch&gt;=1.1</p></li>
<li><p>tqdm</p></li>
</ul>
</div>
</div>
<div class="section" id="calibration-metrics">
<h1><a class="toc-backref" href="#id52">Calibration Metrics</a><a class="headerlink" href="#calibration-metrics" title="Permalink to this headline">¶</a></h1>
<p>The most common metric to determine miscalibration in the scope of classification is the <em>Expected Calibration Error</em> (ECE) <a class="footnote-reference brackets" href="#id35" id="id6">1</a>. This metric divides the confidence space into several bins and measures the observed accuracy in each bin. The bin gaps between observed accuracy and bin confidence are summed up and weighted by the amount of samples in each bin. The <em>Maximum Calibration Error</em> (MCE) denotes the highest gap over all bins. The <em>Average Calibration Error</em> (ACE) <a class="footnote-reference brackets" href="#id45" id="id7">11</a> denotes the average miscalibration where each bin gets weighted equally.
For object detection, we implemented the <em>Detection Calibration Error</em> (D-ECE) <a class="footnote-reference brackets" href="#id46" id="id8">12</a> that is the natural extension of the ECE to object detection tasks. The miscalibration is determined w.r.t. the bounding box information provided (e.g. box location and/or scale). For this purpose, all available information gets binned in a multidimensional histogram. The accuracy is then calculated in each bin separately to determine the mean deviation between confidence and accuracy.</p>
<ul class="simple">
<li><p>(Detection) Expected Calibration Error <a class="footnote-reference brackets" href="#id35" id="id9">1</a>, <a class="footnote-reference brackets" href="#id46" id="id10">12</a> (<em>netcal.metrics.ECE</em>)</p></li>
<li><p>(Detection) Maximum Calibration Error <a class="footnote-reference brackets" href="#id35" id="id11">1</a>, <a class="footnote-reference brackets" href="#id46" id="id12">12</a>  (<em>netcal.metrics.MCE</em>)</p></li>
<li><p>(Detection) Average Calibration Error <a class="footnote-reference brackets" href="#id45" id="id13">11</a>, <a class="footnote-reference brackets" href="#id46" id="id14">12</a> (<em>netcal.metrics.ACE</em>)</p></li>
</ul>
</div>
<div class="section" id="methods">
<h1><a class="toc-backref" href="#id53">Methods</a><a class="headerlink" href="#methods" title="Permalink to this headline">¶</a></h1>
<p>The calibration methods are separated into binning and scaling methods. The binning methods divide the available information into several bins (like ECE or D-ECE) and perform calibration on each bin. The scaling methods scale the confidence estimates or logits directly to calibrated confidence estimates - on detection calibration, this is done w.r.t. the additional regression branch of a network.</p>
<p>Important: if you use the detection mode, you need to specifiy the flag “detection=True” in the constructor of the according method (this is not necessary for <em>netcal.scaling.LogisticCalibrationDependent</em> and <em>netcal.scaling.BetaCalibrationDependent</em>).</p>
<p>Most of the calibration methods are designed for binary classification tasks. For binning methods, multi-class calibration is performed in “one vs. all” by default.</p>
<p>Some methods like “Isotonic Regression” utilize methods from the scikit-learn API <a class="footnote-reference brackets" href="#id43" id="id15">9</a>.</p>
<p>Another group are the regularization tools which are added to the loss during the training of a Neural Network.</p>
<div class="section" id="binning">
<h2><a class="toc-backref" href="#id54">Binning</a><a class="headerlink" href="#binning" title="Permalink to this headline">¶</a></h2>
<p>Implemented binning methods are:</p>
<ul class="simple">
<li><p>Histogram Binning for classification <a class="footnote-reference brackets" href="#id37" id="id16">3</a>, <a class="footnote-reference brackets" href="#id38" id="id17">4</a> and object detection <a class="footnote-reference brackets" href="#id46" id="id18">12</a> (<em>netcal.binning.HistogramBinning</em>)</p></li>
<li><p>Isotonic Regression <a class="footnote-reference brackets" href="#id38" id="id19">4</a>, <a class="footnote-reference brackets" href="#id39" id="id20">5</a> (<em>netcal.binning.IsotonicRegression</em>)</p></li>
<li><p>Bayesian Binning into Quantiles (BBQ) <a class="footnote-reference brackets" href="#id35" id="id21">1</a> (<em>netcal.binning.BBQ</em>)</p></li>
<li><p>Ensemble of Near Isotonic Regression (ENIR) <a class="footnote-reference brackets" href="#id40" id="id22">6</a> (<em>netcal.binning.ENIR</em>)</p></li>
</ul>
</div>
<div class="section" id="scaling">
<h2><a class="toc-backref" href="#id55">Scaling</a><a class="headerlink" href="#scaling" title="Permalink to this headline">¶</a></h2>
<p>Implemented scaling methods are:</p>
<ul class="simple">
<li><p>Logistic Calibration/Platt Scaling for classification <a class="footnote-reference brackets" href="#id44" id="id23">10</a>, <a class="footnote-reference brackets" href="#id46" id="id24">12</a> and object detection <a class="footnote-reference brackets" href="#id46" id="id25">12</a> (<em>netcal.scaling.LogisticCalibration</em>)</p></li>
<li><p>Dependent Logistic Calibration for object detection <a class="footnote-reference brackets" href="#id46" id="id26">12</a> (<em>netcal.scaling.LogisticCalibrationDependent</em>) - on detection, this method is able to capture correlations between all input quantities and should be preferred over Logistic Calibration for object detection</p></li>
<li><p>Temperature Scaling for classification <a class="footnote-reference brackets" href="#id41" id="id27">7</a> and object detection <a class="footnote-reference brackets" href="#id46" id="id28">12</a> (<em>netcal.scaling.TemperatureScaling</em>)</p></li>
<li><p>Beta Calibration for classification <a class="footnote-reference brackets" href="#id36" id="id29">2</a> and object detection <a class="footnote-reference brackets" href="#id46" id="id30">12</a> (<em>netcal.scaling.BetaCalibration</em>)</p></li>
<li><p>Dependent Beta Calibration for object detection <a class="footnote-reference brackets" href="#id46" id="id31">12</a> (<em>netcal.scaling.BetaCalibrationDependent</em>) - on detection, this method is able to capture correlations between all input quantities and should be preferred over Beta Calibration for object detection</p></li>
</ul>
</div>
<div class="section" id="regularization">
<h2><a class="toc-backref" href="#id56">Regularization</a><a class="headerlink" href="#regularization" title="Permalink to this headline">¶</a></h2>
<p>Implemented regularization methods are:</p>
<ul class="simple">
<li><p>Confidence Penalty <a class="footnote-reference brackets" href="#id42" id="id32">8</a> (<em>netcal.regularization.confidence_penalty</em>)</p></li>
</ul>
</div>
</div>
<div class="section" id="visualization">
<h1><a class="toc-backref" href="#id57">Visualization</a><a class="headerlink" href="#visualization" title="Permalink to this headline">¶</a></h1>
<p>For visualization of miscalibration, one can use a Confidence Histograms &amp; Reliability Diagrams. These diagrams are similar to ECE, the output space is divided into equally spaced bins. The calibration gap between bin accuracy and bin confidence is visualized as a histogram.</p>
<p>On detection calibration, the miscalibration can be visualized either along one additional box information (e.g. the x-position of the predictions) or distributed over two additional box information in terms of a heatmap.</p>
<ul class="simple">
<li><p>Reliability Diagram <a class="footnote-reference brackets" href="#id35" id="id33">1</a>, <a class="footnote-reference brackets" href="#id46" id="id34">12</a> (<em>netcal.presentation.ReliabilityDiagram</em>)</p></li>
</ul>
</div>
<div class="section" id="examples">
<h1><a class="toc-backref" href="#id58">Examples</a><a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h1>
<p>The calibration methods work with the predicted confidence estimates of a neural network and on detection also with the bounding box regression branch.</p>
<div class="section" id="classification">
<h2><a class="toc-backref" href="#id59">Classification</a><a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<p>This is a basic example which uses softmax predictions of a classification task with 10 classes and the given NumPy arrays:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ground_truth</span>  <span class="c1"># this is a NumPy 1-D array with ground truth digits between 0-9 - shape: (n_samples,)</span>
<span class="n">confidences</span>   <span class="c1"># this is a NumPy 2-D array with confidence estimates between 0-1 - shape: (n_samples, n_classes)</span>
</pre></div>
</div>
<p>This is an example for <em>netcal.scaling.TemperatureScaling</em> but also works for every calibration method (remind different constructor parameters):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">netcal.scaling</span> <span class="kn">import</span> <span class="n">TemperatureScaling</span>

<span class="n">temperature</span> <span class="o">=</span> <span class="n">TemperatureScaling</span><span class="p">()</span>
<span class="n">temperature</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">confidences</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
<span class="n">calibrated</span> <span class="o">=</span> <span class="n">temperature</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">confidences</span><span class="p">)</span>
</pre></div>
</div>
<p>The miscalibration can be determined with the ECE:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">netcal.metrics</span> <span class="kn">import</span> <span class="n">ECE</span>

<span class="n">n_bins</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">ece</span> <span class="o">=</span> <span class="n">ECE</span><span class="p">(</span><span class="n">n_bins</span><span class="p">)</span>
<span class="n">uncalibrated_score</span> <span class="o">=</span> <span class="n">ece</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="n">confidences</span><span class="p">)</span>
<span class="n">calibrated_score</span> <span class="o">=</span> <span class="n">ece</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="n">calibrated</span><span class="p">)</span>
</pre></div>
</div>
<p>The miscalibration can be visualized with a Reliability Diagram:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">netcal.presentation</span> <span class="kn">import</span> <span class="n">ReliabilityDiagram</span>

<span class="n">n_bins</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">diagram</span> <span class="o">=</span> <span class="n">ReliabilityDiagram</span><span class="p">(</span><span class="n">n_bins</span><span class="p">)</span>
<span class="n">diagram</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">confidences</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>  <span class="c1"># visualize miscalibration of uncalibrated</span>
<span class="n">diagram</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">calibrated</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>   <span class="c1"># visualize miscalibration of calibrated</span>
</pre></div>
</div>
</div>
<div class="section" id="detection">
<h2><a class="toc-backref" href="#id60">Detection</a><a class="headerlink" href="#detection" title="Permalink to this headline">¶</a></h2>
<p>This is a basic example which uses softmax predictions of a classification task with 10 classes and the given NumPy arrays:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">matched</span>                <span class="c1"># binary NumPy 1-D array (0, 1) that indicates if a bounding box has matched a ground truth at a certain IoU with the right label - shape: (n_samples,)</span>
<span class="n">confidences</span>            <span class="c1"># NumPy 1-D array with confidence estimates between 0-1 - shape: (n_samples,)</span>
<span class="n">relative_x_position</span>    <span class="c1"># NumPy 1-D array with relative center-x position between 0-1 of each prediction - shape: (n_samples,)</span>
</pre></div>
</div>
<p>This is an example for <em>netcal.scaling.LogisticCalibration</em> and <em>netcal.scaling.LogisticCalibrationDependent</em> but also works for every calibration method (remind different constructor parameters):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">netcal.scaling</span> <span class="kn">import</span> <span class="n">LogisticCalibration</span><span class="p">,</span> <span class="n">LogisticCalibrationDependent</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">confidences</span><span class="p">,</span> <span class="n">relative_x_position</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticCalibration</span><span class="p">(</span><span class="n">detection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>        <span class="c1"># flag &#39;detection=True&#39; is mandatory for this method</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">matched</span><span class="p">)</span>
<span class="n">calibrated</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="n">lr_dependent</span> <span class="o">=</span> <span class="n">LogisticCalibrationDependent</span><span class="p">()</span>   <span class="c1"># flag &#39;detection=True&#39; is not necessary as this method is only defined for detection</span>
<span class="n">lr_dependent</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">matched</span><span class="p">)</span>
<span class="n">calibrated</span> <span class="o">=</span> <span class="n">lr_dependent</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>The miscalibration can be determined with the D-ECE:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">netcal.metrics</span> <span class="kn">import</span> <span class="n">ECE</span>

<span class="n">n_bins</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">input_calibrated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">calibrated</span><span class="p">,</span> <span class="n">relative_x_position</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">ece</span> <span class="o">=</span> <span class="n">ECE</span><span class="p">(</span><span class="n">n_bins</span><span class="p">,</span> <span class="n">detection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>           <span class="c1"># flag &#39;detection=True&#39; is mandatory for this method</span>
<span class="n">uncalibrated_score</span> <span class="o">=</span> <span class="n">ece</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">matched</span><span class="p">)</span>
<span class="n">calibrated_score</span> <span class="o">=</span> <span class="n">ece</span><span class="o">.</span><span class="n">measure</span><span class="p">(</span><span class="n">input_calibrated</span><span class="p">,</span> <span class="n">matched</span><span class="p">)</span>
</pre></div>
</div>
<p>The miscalibration can be visualized with a Reliability Diagram:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">netcal.presentation</span> <span class="kn">import</span> <span class="n">ReliabilityDiagram</span>

<span class="n">n_bins</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="n">diagram</span> <span class="o">=</span> <span class="n">ReliabilityDiagram</span><span class="p">(</span><span class="n">n_bins</span><span class="p">,</span> <span class="n">detection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>    <span class="c1"># flag &#39;detection=True&#39; is mandatory for this method</span>
<span class="n">diagram</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">matched</span><span class="p">)</span>                <span class="c1"># visualize miscalibration of uncalibrated</span>
<span class="n">diagram</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_calibrated</span><span class="p">,</span> <span class="n">matched</span><span class="p">)</span>     <span class="c1"># visualize miscalibration of calibrated</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="references">
<h1><a class="toc-backref" href="#id61">References</a><a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<dl class="footnote brackets">
<dt class="label" id="id35"><span class="brackets">1</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id9">2</a>,<a href="#id11">3</a>,<a href="#id21">4</a>,<a href="#id33">5</a>)</span></dt>
<dd><p>Naeini, Mahdi Pakdaman, Gregory Cooper, and Milos Hauskrecht: “Obtaining well calibrated probabilities using bayesian binning.” Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">2</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id29">2</a>)</span></dt>
<dd><p>Kull, Meelis, Telmo Silva Filho, and Peter Flach: “Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers.” Artificial Intelligence and Statistics, PMLR 54:623-631, 2017.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">3</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id16">2</a>)</span></dt>
<dd><p>Zadrozny, Bianca and Elkan, Charles: “Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.” In ICML, pp. 609–616, 2001.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">4</span><span class="fn-backref">(<a href="#id17">1</a>,<a href="#id19">2</a>)</span></dt>
<dd><p>Zadrozny, Bianca and Elkan, Charles: “Transforming classifier scores into accurate multiclass probability estimates.” In KDD, pp. 694–699, 2002.</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id20">5</a></span></dt>
<dd><p>Ryan J Tibshirani, Holger Hoefling, and Robert Tibshirani: “Nearly-isotonic regression.” Technometrics, 53(1):54–61, 2011.</p>
</dd>
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id22">6</a></span></dt>
<dd><p>Naeini, Mahdi Pakdaman, and Gregory F. Cooper: “Binary classifier calibration using an ensemble of near isotonic regression models.” 2016 IEEE 16th International Conference on Data Mining (ICDM). IEEE, 2016.</p>
</dd>
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id27">7</a></span></dt>
<dd><p>Chuan Guo, Geoff Pleiss, Yu Sun and Kilian Q. Weinberger: “On Calibration of Modern Neural Networks.” Proceedings of the 34th International Conference on Machine Learning, 2017.</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id32">8</a></span></dt>
<dd><p>Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L. and Hinton, G.: “Regularizing neural networks by penalizing confident output distributions.” CoRR, 2017.</p>
</dd>
<dt class="label" id="id43"><span class="brackets"><a class="fn-backref" href="#id15">9</a></span></dt>
<dd><p>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M. and Duchesnay, E.: “Scikit-learn: Machine Learning in Python.” In Journal of Machine Learning Research, volume 12 pp 2825-2830, 2011.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">10</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id23">2</a>)</span></dt>
<dd><p>Platt, John: “Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.” Advances in large margin classifiers, 10(3): 61–74, 1999.</p>
</dd>
<dt class="label" id="id45"><span class="brackets">11</span><span class="fn-backref">(<a href="#id7">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>Neumann, Lukas, Andrew Zisserman, and Andrea Vedaldi: “Relaxed Softmax: Efficient Confidence Auto-Calibration for Safe Pedestrian Detection.” Conference on Neural Information Processing Systems (NIPS) Workshop MLITS, 2018.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">12</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id5">2</a>,<a href="#id8">3</a>,<a href="#id10">4</a>,<a href="#id12">5</a>,<a href="#id14">6</a>,<a href="#id18">7</a>,<a href="#id24">8</a>,<a href="#id25">9</a>,<a href="#id26">10</a>,<a href="#id28">11</a>,<a href="#id30">12</a>,<a href="#id31">13</a>,<a href="#id34">14</a>)</span></dt>
<dd><p>Fabian Küppers, Jan Kronenberger, Amirhossein Shantia and Anselm Haselhoff: “Multivariate Confidence Calibration for Object Detection”.” The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, in press, 2020</p>
</dd>
</dl>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">API Reference</a><ul>
</ul>
</li>
<li><a class="reference internal" href="#calibration-framework">Calibration Framework</a></li>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#installation">Installation</a><ul>
<li><a class="reference internal" href="#requirements">Requirements</a></li>
</ul>
</li>
<li><a class="reference internal" href="#calibration-metrics">Calibration Metrics</a></li>
<li><a class="reference internal" href="#methods">Methods</a><ul>
<li><a class="reference internal" href="#binning">Binning</a></li>
<li><a class="reference internal" href="#scaling">Scaling</a></li>
<li><a class="reference internal" href="#regularization">Regularization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#visualization">Visualization</a></li>
<li><a class="reference internal" href="#examples">Examples</a><ul>
<li><a class="reference internal" href="#classification">Classification</a></li>
<li><a class="reference internal" href="#detection">Detection</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>

  <h4>Next topic</h4>
  <p class="topless"><a href="_autosummary/netcal.binning.html"
                        title="next chapter">netcal.binning</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="_autosummary/netcal.binning.html" title="netcal.binning"
             >next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">calibration-framework 1.1.1 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019-2020, Ruhr West University of Applied Sciences, Bottrop, Germany AND Visteon Electronics Germany GmbH, Kerpen, Germany.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.4.
    </div>
  </body>
</html>